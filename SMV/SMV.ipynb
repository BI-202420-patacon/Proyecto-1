{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SMV), Cesar Avellaneda, 2202214746"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto corresponde al noteBook del algoritmo SMV, a cargo de Cesar Avellaneda:\n",
    "\n",
    "Y contiene los siguientes elementos:\n",
    "1. Cargue de los datos.\n",
    "\n",
    "2. Entendimiento de los datos: Describir las características más relevantes de los datos y todo el perfilamiento de datos, incluir el análisis de calidad de datos y hacer una preselección de las variables más importantes para la etapa de modelado.\n",
    "\n",
    "3. Preparación de datos: Solucionar los problemas de calidad de datos previamente identificados que afecten el modelo a construir. Además, debe aplicar todos los proceso de preprocesamiento de datos necesarios para la construcción del modelo de regresión.\n",
    "\n",
    "4. Modelado: Utilizando las variables previamente seleccionadas, construir un modelo de regresión que estime la variable objetivo con el menor error posible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entendimiento del Problema:\n",
    "\n",
    "El problema principal radica en la necesidad de analizar de forma eficiente y automática grandes volúmenes de opiniones en lenguaje natural para relacionarlas con los ODS 3 (Salud y Bienestar), 4 (Educación de Calidad) y 5 (Igualdad de Género). Actualmente, este proceso es manual y requiere la intervención de expertos, lo que lo hace costoso y lento. La oportunidad consiste en desarrollar un modelo analítico automatizado que reduzca el tiempo y los recursos necesarios para este análisis, permitiendo una respuesta más rápida y efectiva a las necesidades de los ciudadanos.\n",
    "\n",
    "# Enfoque Analitico:\n",
    "\n",
    "Categoría de Análisis:\n",
    "El análisis será predictivo y descriptivo. Descriptivo en el sentido de identificar y categorizar opiniones actuales, y predictivo al inferir tendencias y futuras necesidades o áreas de intervención.\n",
    "\n",
    "Tipo y Tarea de Aprendizaje:\n",
    "Aprendizaje Supervisado: Para clasificar las opiniones y relacionarlas con los ODS.\n",
    "\n",
    "Técnicas y Algoritmos:\n",
    "Modelos de Clasificación: SVM (Support Vector Machines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Carga de los Datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re, string, unicodedata\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrevistasODS = pd.read_excel('./data/ODScat_345.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Perfilamiento y entendimiento de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entrevistasODS.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrevistasODS.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrevistadosCopy = entrevistasODS.copy()\n",
    "entrevistadosCopy['Conteo'] = [len(x) for x in entrevistadosCopy['Textos_espanol']]\n",
    "#entrevistadosCopy['Moda'] =\n",
    "entrevistadosCopy['Max'] = [[max([len(x) for x in i.split(' ')])][0] for i in entrevistadosCopy['Textos_espanol']]\n",
    "entrevistadosCopy['Min'] = [[min([len(x) for x in i.split(' ')])][0] for i in entrevistadosCopy['Textos_espanol']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrevistadosCopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Limpieza de Datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word is not None:\n",
    "          new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "          new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    new_words=[]\n",
    "    for word in words:\n",
    "        w= word.lower()\n",
    "        new_words.append(w)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word is not None:\n",
    "            new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "            if new_word != '':\n",
    "                new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()    \n",
    "    print(words)\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "            print(\"if \" + new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    spanish_sw = set(stopwords.words('spanish'))\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in spanish_sw:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def preprocessing(words):\n",
    "    words = to_lowercase(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrevistasODS['palabras'] = entrevistasODS['Textos_espanol'].apply(word_tokenize)\n",
    "entrevistasODS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrevistasODS['palabras'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrevistasODS['palabras'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrevistasODS['palabras1']=entrevistasODS['palabras'].apply(preprocessing)\n",
    "entrevistasODS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrevistasODS['palabras'] = entrevistasODS['palabras'].apply(lambda x: ' '.join(map(str, x)))\n",
    "entrevistasODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data = entrevistasODS['palabras'],entrevistasODS['sdg']\n",
    "y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = CountVectorizer(binary=True)\n",
    "X_dummy = dummy.fit_transform(X_data)\n",
    "print(X_dummy.shape)\n",
    "X_dummy.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer()\n",
    "X_count = count.fit_transform(X_data)\n",
    "print(X_count.shape)\n",
    "X_count.toarray()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_count, y_data, test_size=0.3, random_state=42)\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred = svm_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bolsa de Palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_count = count.fit_transform(X_data)\n",
    "# y_pred = svm_model.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly'], 'gamma': ['scale', 'auto']}\n",
    "# grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=2)\n",
    "# grid.fit(X_train, y_train)\n",
    "# print(\"Mejores parámetros:\", grid.best_params_)\n",
    "# y_pred = grid.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data = entrevistasODS['palabras'],entrevistasODS['sdg']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = CountVectorizer(binary=True)\n",
    "X_dummy = dummy.fit_transform(X_data) \n",
    "print(X_dummy.shape)\n",
    "\n",
    "X_dense = X_dummy.toarray()\n",
    "X_df = pd.DataFrame(X_dense, columns=dummy.get_feature_names_out())\n",
    "X_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TfidfTransformer(norm='l2', use_idf=True)\n",
    "tt_matrix = tt.fit_transform(X_dummy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_matrix_dense = tt_matrix.toarray()\n",
    "vocab = dummy.get_feature_names_out()\n",
    "tt_df = pd.DataFrame(np.round(tt_matrix_dense, 2), columns=vocab)\n",
    "tt_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "feature_names = list(tt_df.columns)\n",
    "# build the document frequency matrix\n",
    "df = np.diff(sp.csc_matrix(tt_df, copy=True).indptr)\n",
    "df = 1 + df # adding 1 to smoothen idf later\n",
    "# show smoothened document frequencies\n",
    "pd.DataFrame([df], columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compute inverse document frequencies\n",
    "total_docs = 1 + len(vocab)\n",
    "idf = 1.0 + np.log(float(total_docs) / df)\n",
    "# show smoothened idfs\n",
    "pd.DataFrame([np.round(idf, 2)], columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute idf diagonal matrix\n",
    "total_features = tt_df.shape[1]\n",
    "idf_diag = sp.spdiags(idf, diags=0, m=total_features, n=total_features)\n",
    "idf_dense = idf_diag.todense()\n",
    "# print the idf diagonal matrix\n",
    "pd.DataFrame(np.round(idf_dense, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute tfidf feature matrix\n",
    "tf = np.array(tt_df, dtype=\"float64\")\n",
    "tfidf = tf * idf\n",
    "# view raw tfidf feature matrix\n",
    "tfidf = pd.DataFrame(np.round(tfidf, 2), columns=feature_names)\n",
    "tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf, y_data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Entrenamos el clasificador Naive Bayes con la matriz TF-IDF\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Hacemos predicciones en el conjunto de prueba\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculamos la precisión\n",
    "print(f\"Precisión: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "\n",
    "# Mostramos el reporte de clasificación\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árbol de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# Entrenamos el clasificador Decision Tree con la matriz TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(entrevistasODS['palabras'])\n",
    "Y=entrevistasODS['sdg']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "clf = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "print('Exactitud: %.2f' % accuracy_score(y_test, y_pred))\n",
    "print(\"Recall: {}\".format(recall_score(y_test,y_pred, average='weighted')))\n",
    "print(\"Precisión: {}\".format(precision_score(y_test,y_pred, average='weighted')))\n",
    "print(\"Puntuación F1: {}\".format(f1_score(y_test,y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
